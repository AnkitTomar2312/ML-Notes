{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 14) (127, 13)\n",
      "-0.00020971351407792822 2.4560090955864703e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import model_selection,linear_model\n",
    "trainData=np.genfromtxt(\"./assets/Gradient_Descent/training_coston_x_y_train.csv\",delimiter=\",\")\n",
    "testData=np.genfromtxt(\"./assets/Gradient_Descent/test_coston_x_test.csv\",delimiter=\",\")\n",
    "print(trainData.shape,testData.shape)\n",
    "#step_gradient function\n",
    "def step_gradient(points,alpha,m,c):\n",
    "    m_slope=0\n",
    "    c_slope=0\n",
    "    M=len(points)\n",
    "    for i in range(M):\n",
    "        x=points[i,0]\n",
    "        y=points[i,1]\n",
    "        m_slope+=(-2/M)*(y-m*x-c)*x\n",
    "        c_slope+=(-2/M)*(y-m*x-c)\n",
    "    new_m=m-alpha*m_slope\n",
    "    new_c=c-alpha*c_slope\n",
    "    return new_m,new_c\n",
    "#gd function\n",
    "def gd(points,alpha,num_ite):\n",
    "    m=0\n",
    "    c=0\n",
    "    for i in range(num_ite):\n",
    "        m,c =step_gradient(points,alpha,m,c)\n",
    "    return m,c\n",
    "#Cost Function\n",
    "def cost(points,m,c):\n",
    "    total_cost=0\n",
    "    M=len(points)\n",
    "    for i in range(M):\n",
    "        x=points[i,0]\n",
    "        y=points[i,1]\n",
    "        total_cost+=(1/M)*((y-m*x-c)**2)\n",
    "    return total_cost\n",
    "def run():\n",
    "    data=trainData\n",
    "    alpha=0.0001\n",
    "    num_ite=5\n",
    "    m,c=gd(data,alpha,num_ite)\n",
    "    print(m,c)\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 14) (127, 13)\n",
      "Iteration 0, Cost: 599.1222691292876\n",
      "Iteration 100, Cost: 96.79991716378038\n",
      "Iteration 200, Cost: 35.0922936913772\n",
      "Iteration 300, Cost: 26.365427913904405\n",
      "Iteration 400, Cost: 24.794496311005183\n",
      "Iteration 500, Cost: 24.321632271325647\n",
      "Iteration 600, Cost: 24.083442149339046\n",
      "Iteration 700, Cost: 23.930987842083713\n",
      "Iteration 800, Cost: 23.825018976402376\n",
      "Iteration 900, Cost: 23.74861093692395\n",
      "Predictions saved to boston_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Load Training and Test Data\n",
    "train_data = np.genfromtxt(\"./assets/training_coston_x_y_train.csv\", delimiter=\",\")\n",
    "test_data = np.genfromtxt(\"./assets/test_coston_x_test.csv\", delimiter=\",\")\n",
    "\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "# Step 2: Split into Features (X) and Target (Y)\n",
    "X_train = train_data[:, :-1]  # All columns except the last one (Features)\n",
    "Y_train = train_data[:, -1]   # Last column (Target)\n",
    "X_test = test_data            # Test features only\n",
    "\n",
    "# Step 3: Initialize Parameters\n",
    "m = np.zeros(X_train.shape[1])  # One weight for each feature\n",
    "c = 0                           # cias term\n",
    "alpha = 0.01                    # Learning rate\n",
    "num_iterations = 1000           # Numcer of iterations\n",
    "\n",
    "# Step 4: Gradient Descent Loop\n",
    "N = len(Y_train)  # Numcer of data points\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    # Predictions\n",
    "    predictions = np.dot(X_train, m) + c  \n",
    "    \n",
    "    # Calculate Error\n",
    "    error = predictions - Y_train  \n",
    "    \n",
    "    # Update Gradients for Weights and cias\n",
    "    m_gradient = (1 / N) * np.dot(X_train.T, error)\n",
    "    c_gradient = (1 / N) * np.sum(error)\n",
    "    \n",
    "    # Update Weights and cias\n",
    "    m -= alpha * m_gradient\n",
    "    c -= alpha * c_gradient\n",
    "    \n",
    "    # Print cost every 100 iterations\n",
    "    if i % 100 == 0:\n",
    "        cost = np.mean(error ** 2)\n",
    "        print(f\"Iteration {i}, Cost: {cost}\")\n",
    "\n",
    "# Step 5: Make Predictions on Test Data\n",
    "predictions = np.dot(X_test, m) + c\n",
    "\n",
    "# Step 6: Save Predictions to CSV\n",
    "#np.savetxt(\"coston_predictions.csv\", predictions, fmt=\"%.2f\", delimiter=\",\")\n",
    "print(\"Predictions saved to coston_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(379, 14) (127, 13)\n",
      "Iteration 0, Cost: 0.9972491249701807\n",
      "Iteration 10, Cost: 0.9836109355466636\n",
      "Iteration 20, Cost: 0.9749991418721631\n",
      "Iteration 30, Cost: 0.9695608048202178\n",
      "Iteration 40, Cost: 0.9661262041299857\n",
      "Iteration 50, Cost: 0.9639568715234804\n",
      "Iteration 60, Cost: 0.962586564015994\n",
      "Iteration 70, Cost: 0.9617208901768647\n",
      "Iteration 80, Cost: 0.9611739530602107\n",
      "Iteration 90, Cost: 0.9608283560628497\n",
      "Iteration 100, Cost: 0.9606099550082374\n",
      "Iteration 110, Cost: 0.9604719182998951\n",
      "Iteration 120, Cost: 0.9603846628461853\n",
      "Iteration 130, Cost: 0.9603294993387341\n",
      "Iteration 140, Cost: 0.9602946193927218\n",
      "Iteration 150, Cost: 0.9602725613042037\n",
      "Iteration 160, Cost: 0.9602586094557497\n",
      "Iteration 170, Cost: 0.9602497833065154\n",
      "Iteration 180, Cost: 0.9602441987275357\n",
      "Iteration 190, Cost: 0.9602406645085153\n",
      "Iteration 200, Cost: 0.9602384274120207\n",
      "Iteration 210, Cost: 0.9602370110678984\n",
      "Iteration 220, Cost: 0.9602361141546663\n",
      "Iteration 230, Cost: 0.9602355460417964\n",
      "Iteration 240, Cost: 0.9602351861047526\n",
      "Iteration 250, Cost: 0.9602349580014582\n",
      "Iteration 260, Cost: 0.9602348134057995\n",
      "Iteration 270, Cost: 0.9602347217197181\n",
      "Iteration 280, Cost: 0.960234663565389\n",
      "Iteration 290, Cost: 0.9602346266678554\n",
      "Iteration 300, Cost: 0.9602346032495347\n",
      "Iteration 310, Cost: 0.9602345883811488\n",
      "Iteration 320, Cost: 0.9602345789377502\n",
      "Iteration 330, Cost: 0.9602345729376783\n",
      "Iteration 340, Cost: 0.9602345691239013\n",
      "Iteration 350, Cost: 0.9602345666987874\n",
      "Iteration 360, Cost: 0.9602345651560411\n",
      "Iteration 370, Cost: 0.9602345641741801\n",
      "Iteration 380, Cost: 0.9602345635489977\n",
      "Iteration 390, Cost: 0.960234563150733\n",
      "Iteration 400, Cost: 0.9602345628968959\n",
      "Iteration 410, Cost: 0.9602345627350257\n",
      "Iteration 420, Cost: 0.9602345626317488\n",
      "Iteration 430, Cost: 0.9602345625658195\n",
      "Iteration 440, Cost: 0.960234562523705\n",
      "Iteration 450, Cost: 0.9602345624967886\n",
      "Iteration 460, Cost: 0.9602345624795756\n",
      "Iteration 470, Cost: 0.9602345624685604\n",
      "Iteration 480, Cost: 0.9602345624615066\n",
      "Iteration 490, Cost: 0.9602345624569864\n",
      "Iteration 500, Cost: 0.9602345624540896\n",
      "Iteration 510, Cost: 0.9602345624522296\n",
      "Iteration 520, Cost: 0.9602345624510358\n",
      "Iteration 530, Cost: 0.9602345624502676\n",
      "Iteration 540, Cost: 0.9602345624497739\n",
      "Iteration 550, Cost: 0.9602345624494572\n",
      "Iteration 560, Cost: 0.960234562449252\n",
      "Iteration 570, Cost: 0.9602345624491199\n",
      "Iteration 580, Cost: 0.9602345624490354\n",
      "Iteration 590, Cost: 0.9602345624489811\n",
      "Iteration 600, Cost: 0.9602345624489447\n",
      "Iteration 610, Cost: 0.960234562448922\n",
      "Iteration 620, Cost: 0.9602345624489059\n",
      "Iteration 630, Cost: 0.9602345624488972\n",
      "Iteration 640, Cost: 0.9602345624488914\n",
      "Iteration 650, Cost: 0.9602345624488872\n",
      "Iteration 660, Cost: 0.9602345624488854\n",
      "Iteration 670, Cost: 0.9602345624488824\n",
      "Iteration 680, Cost: 0.960234562448882\n",
      "Iteration 690, Cost: 0.9602345624488813\n",
      "Iteration 700, Cost: 0.96023456244888\n",
      "Iteration 710, Cost: 0.9602345624488805\n",
      "Iteration 720, Cost: 0.9602345624488801\n",
      "Iteration 730, Cost: 0.9602345624488797\n",
      "Iteration 740, Cost: 0.9602345624488798\n",
      "Iteration 750, Cost: 0.9602345624488793\n",
      "Iteration 760, Cost: 0.9602345624488798\n",
      "Iteration 770, Cost: 0.9602345624488798\n",
      "Iteration 780, Cost: 0.9602345624488795\n",
      "Iteration 790, Cost: 0.9602345624488792\n",
      "Iteration 800, Cost: 0.960234562448879\n",
      "Iteration 810, Cost: 0.9602345624488803\n",
      "Iteration 820, Cost: 0.96023456244888\n",
      "Iteration 830, Cost: 0.9602345624488796\n",
      "Iteration 840, Cost: 0.9602345624488795\n",
      "Iteration 850, Cost: 0.9602345624488791\n",
      "Iteration 860, Cost: 0.9602345624488797\n",
      "Iteration 870, Cost: 0.9602345624488793\n",
      "Iteration 880, Cost: 0.9602345624488797\n",
      "Iteration 890, Cost: 0.9602345624488792\n",
      "Iteration 900, Cost: 0.9602345624488795\n",
      "Iteration 910, Cost: 0.9602345624488796\n",
      "Iteration 920, Cost: 0.9602345624488802\n",
      "Iteration 930, Cost: 0.9602345624488792\n",
      "Iteration 940, Cost: 0.9602345624488791\n",
      "Iteration 950, Cost: 0.9602345624488795\n",
      "Iteration 960, Cost: 0.9602345624488795\n",
      "Iteration 970, Cost: 0.9602345624488805\n",
      "Iteration 980, Cost: 0.9602345624488791\n",
      "Iteration 990, Cost: 0.9602345624488794\n",
      "Final values: m = -0.18464734471512378, c = 0.006079610811539988\n",
      "Final cost: 0.96023456244888\n",
      "Predictions saved to 'predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load Data\n",
    "trainData = np.genfromtxt(\"./assets/training_boston_x_y_train.csv\", delimiter=\",\")\n",
    "testData = np.genfromtxt(\"./assets/test_boston_x_test.csv\", delimiter=\",\")\n",
    "\n",
    "# Print Data Shape\n",
    "print(trainData.shape, testData.shape)\n",
    "\n",
    "# Step Gradient Function\n",
    "def step_gradient_descent(X, Y, m, c, learning_rate):\n",
    "    m_slope = 0\n",
    "    c_slope = 0\n",
    "    M = len(X)\n",
    "    \n",
    "    for i in range(M):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        m_slope += (-2 / M) * (y - (m * x + c)) * x\n",
    "        c_slope += (-2 / M) * (y - (m * x + c))\n",
    "    \n",
    "    m = m - learning_rate * m_slope\n",
    "    c = c - learning_rate * c_slope\n",
    "    \n",
    "    return m, c\n",
    "\n",
    "# Gradient Descent Function\n",
    "def gradient_descent(X, Y, learning_rate, iterations):\n",
    "    m = 0  # Initial slope\n",
    "    c = 0  # Initial intercept\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        m, c = step_gradient_descent(X, Y, m, c, learning_rate)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Iteration {i}, Cost: {cost_func(X, Y, m, c)}\")\n",
    "    \n",
    "    return m, c\n",
    "\n",
    "# Cost Function\n",
    "def cost_func(X, Y, m, c):\n",
    "    total_cost = 0\n",
    "    M = len(X)\n",
    "    \n",
    "    for i in range(M):\n",
    "        x = X[i]\n",
    "        y = Y[i]\n",
    "        total_cost += (1 / M) * ((y - (m * x + c)) ** 2)\n",
    "    \n",
    "    return total_cost\n",
    "\n",
    "# Run Function\n",
    "def run():\n",
    "    # Extract X and Y from trainData\n",
    "    X = trainData[:, 0]  # Feature column\n",
    "    Y = trainData[:, 1]  # Target column\n",
    "    learning_rate = 0.01\n",
    "    iterations = 1000\n",
    "    \n",
    "    m, c = gradient_descent(X, Y, learning_rate, iterations)\n",
    "    \n",
    "    print(f\"Final values: m = {m}, c = {c}\")\n",
    "    print(f\"Final cost: {cost_func(X, Y, m, c)}\")\n",
    "    \n",
    "    # Predictions on Test Data\n",
    "    predictions = m * testData + c\n",
    "    #np.savetxt(\"predictions.csv\", predictions, fmt='%.2f')\n",
    "    print(\"Predictions saved to 'predictions.csv'\")\n",
    "\n",
    "# Run the program\n",
    "run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete. Predictions saved to boston_predictions.csv.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load data\n",
    "trainData = np.genfromtxt(\"./assets/training_boston_x_y_train.csv\", delimiter=\",\")\n",
    "testData = np.genfromtxt(\"./assets/test_boston_x_test.csv\", delimiter=\",\")\n",
    "\n",
    "# Separate features and target\n",
    "X = trainData[:, :-1]\n",
    "Y = trainData[:, -1]\n",
    "\n",
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "testData = scaler.transform(testData)\n",
    "\n",
    "# Initialize Parameters\n",
    "m = np.zeros(X.shape[1])  # Weights for all features\n",
    "\n",
    "c = 0  # Bias\n",
    "\n",
    "# Cost Function\n",
    "def cost_function(X, Y, m, c):\n",
    "    N = len(Y)\n",
    "    predictions = np.dot(X, m) + c\n",
    "    cost = (1 / (2 * N)) * np.sum((predictions - Y) ** 2)\n",
    "    return cost\n",
    "\n",
    "# Gradient Descent\n",
    "def gradient_descent(X, Y, m, c, learning_rate, num_iterations):\n",
    "    N = len(Y)\n",
    "    for _ in range(num_iterations):\n",
    "        predictions = np.dot(X, m) + c\n",
    "        error = predictions - Y\n",
    "        m_gradient = (1 / N) * np.dot(X.T, error)\n",
    "        b_gradient = (1 / N) * np.sum(error)\n",
    "        m -= learning_rate * m_gradient\n",
    "        c -= learning_rate * b_gradient\n",
    "    return m, c\n",
    "\n",
    "# Train the Model\n",
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "m, c = gradient_descent(X, Y, m, c, learning_rate, num_iterations)\n",
    "\n",
    "# Make Predictions on Test Data\n",
    "test_predictions = np.dot(testData, m) + c\n",
    "\n",
    "# Save Predictions\n",
    "#np.savetxt(\"boston_predictions.csv\", test_predictions, delimiter=\",\", fmt='%.2f')\n",
    "\n",
    "print(\"Model training complete. Predictions saved to boston_predictions.csv.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
